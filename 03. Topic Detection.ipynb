{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic detection or identification is the process of discovering topics that are present in the input document set. These topics can be multiple words that occur uniquely in a given text.\n",
    "\n",
    "In this part we will use Latent Dirichlet allocation algorithm to identify topics in a given input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicDetection:\n",
    "    \n",
    "    \"\"\"\n",
    "    (1) - define a function getDocuments(), whose responsability is to download few documents using feedparser\n",
    "        - create an empty list to save all RSS 'summary' text information\n",
    "        - if 'summary' contains sensitive words (e.g 'ex') we skip it\n",
    "        - append summary text(s) to the new list for later use\n",
    "    \"\"\"\n",
    "    def getDocuments(self):\n",
    "        \"\"\"\n",
    "        Method to get a list of documents for topic detection.\n",
    "        \"\"\"\n",
    "        url = 'https://sports.yahoo.com/mlb/rss.xml'\n",
    "        # parse the url using feedparser library\n",
    "        feed = feedparser.parse(url)\n",
    "        self.documents = []\n",
    "        \n",
    "        i = 0\n",
    "        # iterate over the top 5 documents \n",
    "        for entry in feed['entries'][:5]:\n",
    "            i += 1\n",
    "            text = entry['summary']\n",
    "            # skip articles containing sensitive words\n",
    "            if 'ex' in text:\n",
    "                continue\n",
    "            self.documents.append(text)\n",
    "            print(\"Document {} : {}\".format(i, text))\n",
    "        print('INFO: Fetching documents from {} completed'.format(url))\n",
    "        \n",
    "    \"\"\"\n",
    "    (2) - define a function to apply simple preprocessing \n",
    "        - apply lowercase, tokenization, remove english stopwords\n",
    "        - append the newly generated list to a new list for later use\n",
    "    \"\"\"\n",
    "    def cleanDocuments(self):\n",
    "        \"\"\"\n",
    "        Method to apply preprocessing on input documents.\n",
    "        \"\"\"\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "        # define list of unique stopwords\n",
    "        eng_stop_words = set(stopwords.words('english'))\n",
    "        self.cleaned = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            # apply lowercase \n",
    "            lowercase_doc = doc.lower()\n",
    "            # tokenize document\n",
    "            words = tokenizer.tokenize(lowercase_doc)\n",
    "            # filter out english stopwords\n",
    "            non_stopped_words = [word for word in words if not word in eng_stop_words]\n",
    "            # append the newly generated list to 'cleaned' list\n",
    "            self.cleaned.append(non_stopped_words)\n",
    "        print('INFO: Cleaning {} documents completed'.format(len(self.documents)))\n",
    "        \n",
    "    \"\"\"\n",
    "    (3) - define a function to create Latent Dirichlet model to detect topic from text\n",
    "        - create a dictionary of unique words from cleaned documents\n",
    "        - create corpus as a bag of words for each cleaned sentence\n",
    "        - create a model on the corpus with the number of topics defined as 2 and set the vocabulary\n",
    "        size/mapping using id2word parameter\n",
    "        - print 2 topics, where each topic should contain four words on the screen\n",
    "    \"\"\"\n",
    "    def doLDA(self):\n",
    "        \"\"\"\n",
    "        Method to detect topic from document(s) using Latent Dirichlet allocation algorithm.\n",
    "        \"\"\"\n",
    "        dictionary = corpora.Dictionary(self.cleaned)\n",
    "        # create corpus using cleaned document list\n",
    "        corpus = [dictionary.doc2bow(cleandoc) for cleandoc in self.cleaned]\n",
    "        # create Latent Dirichlet model using the newly generated corpus\n",
    "        ldamodel = models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "        print(ldamodel.print_topics(num_topics=2, num_words=4))\n",
    "        \n",
    "    \"\"\"\n",
    "    (4) - define a function which does all the steps in order: getDocuments(), cleanDocuments(), doLDA()\n",
    "    \"\"\"\n",
    "    def run(self):\n",
    "        self.getDocuments()\n",
    "        self.cleanDocuments()\n",
    "        self.doLDA()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic object\n",
    "topic = TopicDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 : After years of quiet work, the A's shortstop has launched himself on to MVP ballots and put his name alongside superstars like Mike Trout.\n",
      "Document 2 : It was an emotional scene in Seattle for the man they call King.\n",
      "Document 3 : King FÃ©lix held court for the final time in Seattle\n",
      "Document 4 : Bruce Bochy has a chance to finish the San Francisco era of his managerial career with a winning record against the club's chief rival when the Giants host the Los Angeles Dodgers beginning Friday night in the final three games of their skipper's career.  The Dodgers (103-56) will be looking\n",
      "Document 5 : When Christian Yelich fouled a ball off of his kneecap on Sept. 10, the Milwaukee Brewers' season looked doomed.  Staying in the wild-card race was supposed to be tough without the reigning NL MVP, but not only did they stay in it, their scorching September has the Brewers in the hunt for a second\n",
      "INFO: Fetching documents from https://sports.yahoo.com/mlb/rss.xml completed\n"
     ]
    }
   ],
   "source": [
    "# get documents\n",
    "topic.getDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Cleaning 5 documents completed\n"
     ]
    }
   ],
   "source": [
    "# cleaning of documents\n",
    "topic.cleanDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.023*\"mvp\" + 0.019*\"brewers\" + 0.018*\"dodgers\" + 0.018*\"mike\"'), (1, '0.022*\"king\" + 0.022*\"seattle\" + 0.022*\"career\" + 0.021*\"final\"')]\n"
     ]
    }
   ],
   "source": [
    "# detect topic using LDA (Latent Dirichlet Allocation)\n",
    "topic.doLDA()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
